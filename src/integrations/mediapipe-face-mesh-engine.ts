// src/integrations/mediapipe-face-mesh-engine.ts ‚Äì MediaPipe Face Mesh Engine v1.0
// 468 facial landmarks + 3D geometry + blendshape coefficients (emotions)
// Real-time face tracking, valence-weighted micro-expression filtering, mercy-protected output
// WebNN acceleration, offline-capable after first load
// MIT License ‚Äì Autonomicity Games Inc. 2026

import { FaceMesh } from '@mediapipe/face_mesh';
import { currentValence } from '@/core/valence-tracker';
import { mercyGate } from '@/core/mercy-gate';
import mercyHaptic from '@/utils/haptic-patterns';
import visualFeedback from '@/utils/visual-feedback';
import audioFeedback from '@/utils/audio-feedback';

const MEDIAPIPE_FACE_MESH_CONFIG = {
  maxNumFaces: 1,                         // single face focus (can increase to 4)
  refineLandmarks: true,                  // iris tracking + detailed contours
  minDetectionConfidence: 0.7,
  minTrackingConfidence: 0.65
};

const FACE_LANDMARKS = 468;                // 468 points (including iris)
const BLENDSHAPES_COUNT = 52;              // emotion/expression coefficients
const CONFIDENCE_THRESHOLD = 0.78;
const MERCY_FALSE_POSITIVE_DROP = 0.12;

interface FaceMeshResult {
  faceLandmarks: any[];                   // 468 landmarks with x,y,z,visibility,presence
  faceWorldLandmarks: any[];              // real-world 3D coordinates (meters)
  blendshapes: any[];                     // 52 blendshape coefficients (emotions)
  confidence: number;
  isSafe: boolean;
  projectedValenceImpact: number;
  dominantExpression?: string;            // e.g. 'happy', 'sad', 'neutral'
}

let faceMeshDetector: FaceMesh | null = null;
let isInitialized = false;

export class MediaPipeFaceMeshEngine {
  static async initialize(): Promise<void> {
    const actionName = 'Initialize MediaPipe Face Mesh engine';
    if (!await mercyGate(actionName)) return;

    if (isInitialized) {
      console.log("[MediaPipeFaceMeshEngine] Already initialized");
      return;
    }

    console.log("[MediaPipeFaceMeshEngine] Loading Face Mesh (468 landmarks)...");

    try {
      faceMeshDetector = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });

      await faceMeshDetector.setOptions(MEDIAPIPE_FACE_MESH_CONFIG);
      await faceMeshDetector.initialize();

      isInitialized = true;
      mercyHaptic.cosmicHarmony();
      visualFeedback.success({ message: 'Face Mesh engine awakened ‚ö°Ô∏è' });
      audioFeedback.cosmicHarmony();
      console.log("[MediaPipeFaceMeshEngine] Face Mesh fully loaded ‚Äì 468 landmarks + blendshapes ready");
    } catch (err) {
      console.error("[MediaPipeFaceMeshEngine] Initialization failed:", err);
      mercyHaptic.warningPulse();
      visualFeedback.error({ message: 'Face Mesh awakening interrupted ‚ö†Ô∏è' });
    }
  }

  static async detectFaceMesh(videoElement: HTMLVideoElement): Promise<FaceMeshResult | null> {
    if (!isInitialized || !faceMeshDetector) {
      await this.initialize();
      return null;
    }

    const actionName = 'Detect face mesh from video frame';
    if (!await mercyGate(actionName)) return null;

    const valence = currentValence.get();

    try {
      const results = await faceMeshDetector.send({ image: videoElement });

      if (!results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) {
        return {
          faceLandmarks: [],
          faceWorldLandmarks: [],
          blendshapes: [],
          confidence: 0,
          isSafe: true,
          projectedValenceImpact: 0
        };
      }

      const landmarks = results.multiFaceLandmarks[0];
      const worldLandmarks = results.multiFaceWorldLandmarks?.[0] || [];
      const blendshapes = results.faceBlendshapes?.[0]?.categories || [];

      // Average confidence from visibility
      const avgConfidence = landmarks.reduce((sum: number, lm: any) => sum + (lm.visibility || 0), 0) / FACE_LANDMARKS;

      // Simple dominant expression from blendshapes (expand with ML later)
      let dominantExpression = 'neutral';
      if (blendshapes.length > 0) {
        const topBlend = blendshapes.reduce((prev: any, curr: any) => (curr.score > prev.score ? curr : prev));
        if (topBlend.score > 0.4) {
          dominantExpression = topBlend.categoryName || 'neutral';
        }
      }

      const projectedImpact = avgConfidence * valence - 0.5; // simplistic impact estimate
      const isSafe = projectedImpact >= -0.05;

      if (!isSafe) {
        mercyHaptic.warningPulse(valence * 0.7);
        visualFeedback.warning({ message: 'Face mesh detected ‚Äì projected valence impact low ‚ö†Ô∏è' });
      } else if (dominantExpression !== 'neutral') {
        mercyHaptic.gestureDetected(valence);
        visualFeedback.gesture({ message: `Expression: ${dominantExpression} üòä‚ú®` });
        audioFeedback.gestureDetected(valence);
      }

      return {
        faceLandmarks: landmarks,
        faceWorldLandmarks: worldLandmarks,
        blendshapes,
        confidence: avgConfidence,
        isSafe,
        projectedValenceImpact: projectedImpact,
        dominantExpression
      };
    } catch (err) {
      console.warn("[MediaPipeFaceMeshEngine] Detection error:", err);
      return null;
    }
  }

  static async dispose() {
    if (faceMeshDetector) await faceMeshDetector.close();
    isInitialized = false;
    console.log("[MediaPipeFaceMeshEngine] Face Mesh detector closed");
  }
}

export default MediaPipeFaceMeshEngine;
