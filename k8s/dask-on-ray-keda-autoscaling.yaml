---
# 1. KEDA ScaledObject for Ray worker groups (CPU + GPU autoscaling)
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rathor-ray-cpu-workers-scaler
  namespace: rathor-abundance
spec:
  scaleTargetRef:
    name: rathor-ray-cluster
    kind: RayCluster
    apiVersion: ray.io/v1
    subresourceName: cpu-workers  # targets workerGroupSpecs[1]
  minReplicaCount: 5
  maxReplicaCount: 100
  pollingInterval: 15
  cooldownPeriod: 60
  advanced:
    restoreToOriginalReplicaCount: true
    horizontalPodAutoscalerConfig:
      name: ray-cpu-hpa
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 50
            periodSeconds: 60
  triggers:
  - type: cpu
    metadata:
      type: Utilization
      value: "60"  # scale when CPU > 60%
  - type: memory
    metadata:
      type: Utilization
      value: "70"  # scale when memory > 70%
  - type: ray
    metadata:
      type: pendingTasks
      value: "50"  # scale when >50 pending Ray tasks
      address: "ray://rathor-ray-cluster-head-svc:10001"
      namespace: rathor-abundance
      rayCluster: rathor-ray-cluster

---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rathor-ray-gpu-workers-scaler
  namespace: rathor-abundance
spec:
  scaleTargetRef:
    name: rathor-ray-cluster
    kind: RayCluster
    apiVersion: ray.io/v1
    subresourceName: gpu-workers  # targets workerGroupSpecs[0]
  minReplicaCount: 2
  maxReplicaCount: 50
  pollingInterval: 10
  cooldownPeriod: 120
  advanced:
    restoreToOriginalReplicaCount: true
  triggers:
  - type: gpu
    metadata:
      type: Utilization
      value: "70"  # scale when GPU util > 70%
      gpuMetric: utilization  # requires NVIDIA DCGM exporter
  - type: custom
    metadata:
      type: prometheus
      serverAddress: http://prometheus-server.rathor-monitoring.svc.cluster.local
      metricName: ray_pending_tasks
      query: sum(ray_pending_tasks{namespace="rathor-abundance"}) > 100
      threshold: "100"
      unsafeSsl: "true"  # if self-signed

---
# 2. ScaledJob for bursty one-off evolution jobs (optional)
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: rathor-evo-burst-job
  namespace: rathor-abundance
spec:
  jobTargetRef:
    template:
      spec:
        containers:
        - name: evo-worker
          image: rathor-evo-worker:latest  # your custom image with Ray/Dask code
          command: ["python", "-m", "rathor.evo.distributed"]
          env:
          - name: RAY_ADDRESS
            value: "ray://rathor-ray-cluster-head-svc:10001"
          resources:
            limits:
              cpu: "8"
              memory: "32Gi"
              nvidia.com/gpu: "1"
            requests:
              cpu: "4"
              memory: "16Gi"
              nvidia.com/gpu: "1"
  minReplicaCount: 0
  maxReplicaCount: 50
  pollingInterval: 5
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  triggers:
  - type: cpu
    metadata:
      type: Utilization
      value: "50"
  - type: ray
    metadata:
      type: pendingTasks
      value: "200"
      address: "ray://rathor-ray-cluster-head-svc:10001"

---
# 3. Prometheus ServiceMonitor (for KEDA custom metrics)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ray-keda-metrics
  namespace: rathor-abundance
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: ray-head
  endpoints:
  - port: dashboard
    path: /metrics
    interval: 15s
